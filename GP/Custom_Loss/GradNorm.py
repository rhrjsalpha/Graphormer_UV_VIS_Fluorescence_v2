#from Graphormer.GP.Custom_Loss.sdtw_cuda_loss import SoftDTW
from GP.Custom_Loss.SID_loss import sid_loss
import torch.optim as optim
#from tslearn.metrics import SoftDTWLossPyTorch
from GP.Custom_Loss.soft_dtw_cuda import SoftDTW
import torch
import torch.nn as nn


class GradNorm:
    def __init__(self, num_losses, alpha=0.12):
        self.num_losses = num_losses  # Number of different loss functions
        self.alpha = alpha  # Scaling factor for GradNorm
        self.lambdas = nn.Parameter(torch.ones(self.num_losses, dtype=torch.float32, device="cuda"))  # Initialize loss weights
        self.initial_losses = None  # To store initial loss values

    def compute_weights(self, losses, model):
        """
        Compute dynamic weights for each loss based on gradient norms.

        Args:
            losses (list of torch.Tensor): List containing individual loss values.
            model (torch.nn.Module): The neural network model being trained.

        Returns:
            torch.Tensor: Adjusted weights for each loss.
        """
        #device = self.lambdas.device
        #print("device",device)
        # print("losses",losses)
        # losses: Tensor([L1, L2, ...]) or list of scalars
        if isinstance(losses, list):
            losses = torch.stack([l if torch.is_tensor(l) else torch.tensor(l, device=self.lambdas.device)
                                  for l in losses])

        # ğŸ”§ 1) trainable paramsë§Œ ì‚¬ìš©
        params = [p for p in model.parameters() if p.requires_grad]
        if len(params) == 0:
            # í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ê°€ ì—†ìœ¼ë©´ ê· ë“±ê°€ì¤‘ì¹˜ë¡œ ë¦¬í„´
            return torch.ones_like(self.lambdas) / self.num_losses

        grads = []
        for i, loss in enumerate(losses):
            # ğŸ”§ 2) í•´ë‹¹ lossê°€ grad íŠ¸ë™ ì•ˆ íƒ€ë©´ 0ìœ¼ë¡œ ëŒ€ì²´
            if not torch.is_tensor(loss) or not loss.requires_grad:
                grads.append(torch.zeros((), device=self.lambdas.device))
                continue

            g = torch.autograd.grad(
                loss, params,
                retain_graph=True, create_graph=False, allow_unused=True
            )
            g = [gi for gi in g if gi is not None]
            if len(g) == 0:
                grads.append(torch.zeros((), device=self.lambdas.device))
            else:
                grads.append(torch.norm(torch.stack([gi.norm() for gi in g])))

        grads1 = torch.stack(grads).clamp_min(1e-8)

        #print("grads1", grads1.tolist())
        if 0 in grads1.tolist():
            print("0 in grads list ",grads1, grads)

        # Initialize initial losses during the first call
        if self.initial_losses is None:
            self.initial_losses = losses.detach()
        #else:
        #    # ì´ˆê¸° ì†ì‹¤ë³´ë‹¤ í´ ê²½ìš° ì—…ë°ì´íŠ¸
        #    self.initial_losses = torch.where(losses > self.initial_losses, losses.detach(), self.initial_losses)

        # Compute relative losses
        relative_losses = losses / (self.initial_losses + 1e-8)
        r_i = relative_losses / relative_losses.mean()

        # Compute adjusted loss weights
        adjusted_factor = (grads1 / grads1.mean()) * r_i
        loss_weights = self.lambdas * (adjusted_factor ** self.alpha)

        # Normalize weights to maintain the sum to num_losses
        loss_weights = (self.num_losses * loss_weights) / loss_weights.sum()
        loss_weights = torch.clamp(loss_weights, min=1e-3, max=5.0)
        #print("loss_weights",loss_weights)

        #print(loss_weights)
        return loss_weights

    def update_lambdas(self, loss_weights):
        """
        Update the internal lambda parameters with the new loss weights.

        Args:
            loss_weights (torch.Tensor): The newly computed loss weights.
        """
        with torch.no_grad():
            self.lambdas.copy_(loss_weights.detach())
            #self.lambdas = loss_weights.clone().detach()

class GradNorm_new:
    def __init__(self, num_losses, alpha=0.12):
        self.num_losses = num_losses  # ì‚¬ìš©í•  ì†ì‹¤ í•¨ìˆ˜ ê°œìˆ˜
        self.alpha = alpha  # GradNormì˜ ìŠ¤ì¼€ì¼ë§ ì¡°ì • ê³„ìˆ˜
        self.lambdas = nn.Parameter(torch.ones(num_losses, dtype=torch.float32, device="cuda"))  # ì†ì‹¤ ê°€ì¤‘ì¹˜ ë²¡í„° (ì´ˆê¸°ê°’ 1)
        self.initial_losses = None  # ì´ˆê¸° ì†ì‹¤ ì €ì¥

    def compute_weights(self, losses, model):
        """
        Gradient Norm ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì¡°ì • (ì²« ë²ˆì§¸ epochì€ gradientë§Œ ì‚¬ìš©)
        """
        if isinstance(losses, list):
            losses = torch.cat([loss.unsqueeze(0) for loss in losses])

        # ** ìŒìˆ˜ ì†ì‹¤ ë°©ì§€ ë° ì‘ì€ ê°’ ë³´ì •**
        losses = torch.abs(losses) + 1e-8
        print("losses:", losses)

        # 1. **ì²« ë²ˆì§¸ epochì—ì„œëŠ” `grads`ë§Œ ê³ ë ¤**
        if self.initial_losses is None:
            self.initial_losses = losses.clone().detach()
            print("ğŸ”¹ First epoch detected: Using gradients only (no relative loss computation).")

        # 2. **ì†ì‹¤ ë³€í™”ëŸ‰ì„ ê³ ë ¤í•œ `adjusted_losses` ê³„ì‚° (ì²« ë²ˆì§¸ epoch ì œì™¸)**
        if self.initial_losses is not None:
            adjusted_losses = losses / (torch.abs(losses - self.initial_losses) + 1e-8)
        else:
            adjusted_losses = losses.clone()  # ì²« epochì—ì„œëŠ” ì¡°ì • ì—†ì´ ì‚¬ìš©

        adjusted_losses = torch.clamp(adjusted_losses, min=1e-6, max=1e6)  # NaN ë°©ì§€
        print("adjusted_losses:", adjusted_losses)

        # 3. ê° ì†ì‹¤ì˜ ê·¸ë˜ë””ì–¸íŠ¸ L2 norm ê³„ì‚°
        grads = []
        for loss in losses:
            grad = torch.autograd.grad(loss, model.parameters(), retain_graph=True, create_graph=True, allow_unused=False)
            grad_norm = torch.norm(torch.stack([g.norm() if g is not None else torch.tensor(0.0) for g in grad]))
            grads.append(grad_norm)
        grads = torch.stack(grads)
        grads = torch.clamp(grads, min=1e-8, max=1e6)  # NaN ë°©ì§€
        print("grads_GradNorm:", grads)

        # 4. **ì²« ë²ˆì§¸ epochì—ì„œëŠ” `grads`ë§Œ ì‚¬ìš©**
        if self.initial_losses is None:
            adjusted_factor = grads / (grads.sum() + 1e-8)  # ì²« epochì—ì„œëŠ” gradient ë¹„ìœ¨ë§Œ ì‚¬ìš©
        else:
            # 5. ìƒëŒ€ì  ì†ì‹¤ ë¹„ìœ¨ ê³„ì‚° (r_i ìˆ˜ì •)
            r_i = adjusted_losses / (adjusted_losses.mean() + 1e-8)
            r_i = torch.clamp(r_i, min=1e-6, max=1e6)  # NaN ë°©ì§€
            print("relative_losses (r_i):", r_i)

            # 6. GradNorm ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ ê³µì‹ ì ìš©
            adjusted_factor = (grads / grads.mean() + 1e-8) * r_i  # ì²« epoch ì´í›„ì—ëŠ” Lt / (Lt - L0) ë°©ì‹ ì ìš©
            adjusted_factor = torch.clamp(adjusted_factor, min=1e-6, max=1e6)  # NaN ë°©ì§€
            print("adjusted_factor:", adjusted_factor)

        # 7. ì†ì‹¤ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
        loss_weights = self.lambdas * adjusted_factor ** self.alpha

        # 8. Softmax ì •ê·œí™” ì ìš© (ê°€ì¤‘ì¹˜ ì´í•©ì„ 1ë¡œ ìœ ì§€)
        loss_weights = loss_weights / (loss_weights.sum() + 1e-8)

        # 9. ê·¹ë‹¨ì ì¸ ê°’ ë°©ì§€
        if torch.any(loss_weights < 0.01):
            print("âš  Warning: Loss weights too small. Adjusting weights.")
            loss_weights = torch.clamp(loss_weights, min=0.01, max=0.99)
            loss_weights = loss_weights / (loss_weights.sum() + 1e-8)

        if torch.any(torch.isnan(loss_weights) | torch.isinf(loss_weights)):
            print("âš  Warning: Invalid loss weights detected (NaN/Inf). Resetting to equal weights.")
            loss_weights = torch.ones_like(loss_weights) / self.num_losses  # NaN/Inf ë°œìƒ ì‹œ ì´ˆê¸°í™”
            loss_weights = loss_weights / (loss_weights.sum() + 1e-8)

        print("Final loss weights:", loss_weights)
        return loss_weights

    def update_lambdas(self, loss_weights):
        """
        ì†ì‹¤ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
        """
        with torch.no_grad():
            self.lambdas.copy_(loss_weights)


class GradNorm_exp:
    def __init__(self, num_losses, alpha=0.12):
        self.num_losses = num_losses  # ì‚¬ìš©í•  ì†ì‹¤ í•¨ìˆ˜ ê°œìˆ˜
        self.alpha = alpha  # GradNormì˜ ìŠ¤ì¼€ì¼ë§ ì¡°ì • ê³„ìˆ˜
        self.lambdas = nn.Parameter(torch.ones(num_losses, dtype=torch.float32, device="cuda"))  # ì†ì‹¤ ê°€ì¤‘ì¹˜ ë²¡í„° (ì´ˆê¸°ê°’ 1)
        self.initial_losses = None  # ì´ˆê¸° ì†ì‹¤ ì €ì¥

    def compute_weights(self, losses, model):
        """
        Gradient Norm ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì¡°ì • (e^L ë³€í™˜ ë° ìƒëŒ€ì  ì†ì‹¤ ë¹„ìœ¨ ì ìš©)
        """
        for loss in losses:
            grad = torch.autograd.grad(loss, model.parameters(), retain_graph=True, create_graph=True,
                                       allow_unused=True)
            grad_norm = torch.norm(torch.stack([g.norm() if g is not None else torch.tensor(0.0) for g in grad]))
            print("grad_1", grad_norm)

        if isinstance(losses, list):
            #print(type(losses), len(losses))
            #print(losses)
            losses = torch.cat([loss.unsqueeze(0) for loss in losses])

            #print("losses after\n",losses)

        print("losses",losses)
        # 1. e^L ë³€í™˜ ì ìš© (ì†ì‹¤ì„ í•­ìƒ ì–‘ìˆ˜ë¡œ ë³€í™˜)
        exp_losses = torch.exp(losses)
        print("exp_losses", exp_losses)

        # 2. ì´ˆê¸° ì†ì‹¤ ì €ì¥ (ì²« ë²ˆì§¸ epochì—ì„œ ì„¤ì •)
        if self.initial_losses is None:
            self.initial_losses = exp_losses.clone().detach()  # ì´ˆê¸° ì†ì‹¤ì„ ì €ì¥

        # 3. ê° ì†ì‹¤ì˜ ê·¸ë˜ë””ì–¸íŠ¸ L2 norm ê³„ì‚°
        grads = []
        for loss in exp_losses:
            #print(type(loss), loss.shape, loss)
            grad = torch.autograd.grad(loss, model.parameters(), retain_graph=True, create_graph=True, allow_unused=True)
            grad_norm = torch.norm(torch.stack([g.norm() if g is not None else torch.tensor(0.0) for g in grad]))  # L2 norm ê³„ì‚°
            grads.append(grad_norm)
        #print("grads_GradNorm",grads)

        # 4. ëª¨ë“  ì‘ì—…ì˜ í‰ê·  ê·¸ë˜ë””ì–¸íŠ¸ í¬ê¸° ê³„ì‚°
        grads = torch.stack(grads)  # ëª¨ë“  ê·¸ë˜ë””ì–¸íŠ¸ í¬ê¸°ë¥¼ í•˜ë‚˜ì˜ í…ì„œë¡œ ë³€í™˜
        mean_grad = grads.mean() + 1e-8  # 0 ë°©ì§€

        # 5. ìƒëŒ€ì  ì†ì‹¤ ë¹„ìœ¨ ê³„ì‚° (ê·¸ë¦¼ì˜ ê³µì‹ ì ìš©)
        relative_losses = exp_losses / (self.initial_losses + 1e-8)  # ì´ˆê¸° ì†ì‹¤ ëŒ€ë¹„ ë³€í™”ìœ¨
        r_i = relative_losses / (relative_losses.mean() + 1e-8)  # ìƒëŒ€ì  ì†ì‹¤ ë¹„ìœ¨

        # 6. GradNorm ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ ê³µì‹ ì ìš©,
        #print("normal",(grads / mean_grad) * r_i)
        adjusted_factor = torch.log1p((grads / mean_grad) * r_i) # ìƒˆë¡œìš´ì‹ e^loss í•´ì¤€ê²ƒ ë³´ì •
        #print("adj",adjusted_factor)
        loss_weights = self.lambdas * adjusted_factor ** self.alpha
        #loss_weights_1 = self.lambdas * ((grads / mean_grad) * r_i) ** self.alpha  # `r_i` ë°˜ì˜ , ì†ì‹¤ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ (ê¸°ì¡´ì‹)

        # 7. Softmax ì •ê·œí™” ì ìš© (ê°€ì¤‘ì¹˜ ì´í•©ì„ 1ë¡œ ìœ ì§€)
        loss_weights = loss_weights / (loss_weights.sum() + 1e-8)

        if torch.any(loss_weights < 0.01):
            print("âš  Warning: Invalid loss weights detected (0 values found). Adjusting weights.")
            loss_weights[loss_weights <= 0.01] = 0.01  # 0ì¸ ê°’ì€ 1e-8ë¡œ ì„¤ì •
            loss_weights = loss_weights / (loss_weights.sum() + 1e-8)

        elif torch.any(loss_weights > 0.99):
            print("âš  Warning: Invalid loss weights detected (NaN, inf values found). Adjusting weights.")
            loss_weights[torch.isnan(loss_weights) | torch.isinf(loss_weights)] = 0.99  # NaN ë˜ëŠ” infì¸ ê°’ì€ 0.999ë¡œ ì„¤ì •
            loss_weights = loss_weights / (loss_weights.sum() + 1e-8)

        return loss_weights

    def update_lambdas(self, loss_weights):
        """
        ì†ì‹¤ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
        """
        with torch.no_grad():
            self.lambdas.copy_(loss_weights)


if __name__ == '__main__':
    # âœ… ê°„ë‹¨í•œ ëª¨ë¸ ì •ì˜ (ì…ë ¥ 1ê°œ -> ì¶œë ¥ 10ê°œ)
    # âœ… ì‹¤í–‰ ì„¤ì •
    a = "GradNorm"  # "DWA", "DWAWithNormalization", "Uncertainty", "GradNorm", "GradNormWithNormalization" "FirstLossNormalization"ì¤‘ ì„ íƒ

    # âœ… ëª¨ë¸ ì •ì˜
    class SimpleModel(nn.Module):
        def __init__(self):
            super(SimpleModel, self).__init__()
            self.fc = nn.Linear(1, 10)
            self.activation = nn.Softplus()  # í•­ìƒ ì–‘ìˆ˜ ì¶œë ¥

        def forward(self, x):
            return self.activation(self.fc(x))  # í™œì„±í™” í•¨ìˆ˜ ì ìš©


    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = SimpleModel().to(device)
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    # âœ… ì†ì‹¤ í•¨ìˆ˜ ì •ì˜
    SoftDTWLoss = SoftDTW(use_cuda=True, gamma=0.2, bandwidth=None, normalize=True)
    mse_loss = nn.MSELoss()
    mae_loss = nn.L1Loss()
    huber_loss = nn.SmoothL1Loss()


    #def sid_loss_wrapper(model_spectra, target_spectra):
    #    mask = torch.ones_like(model_spectra, dtype=torch.bool).to(device)
    #    SID_LOSS = sid_loss(model_spectra, target_spectra, mask, threshold=1e-6)
    #    return SID_LOSS
    def sid_loss_wrapper(model_spectra, target_spectra):
        mask = torch.ones_like(model_spectra, dtype=torch.bool, device=device)
        # reduction="mean_valid" â†’ ìƒ˜í”Œë³„ ìœ íš¨ê¸¸ì´ë¡œ ë‚˜ëˆˆ ë’¤ ë°°ì¹˜ í‰ê· (ê°€ë³€ ê¸¸ì´ ê³µì •ì„±)
        SID_LOSS = sid_loss(
            model_spectra,
            target_spectra,
            mask,
            eps=1e-6,
            reduction="mean_valid",
        )
        return SID_LOSS


    # âœ… Loss Weighting ì„¤ì •
    if a == "GradNorm":
        loss_modifier = GradNorm(num_losses=5, alpha=0.12)


    # âœ… í•™ìŠµ ê³¼ì •
    num_epochs = 100
    for epoch in range(num_epochs):
        model.train()
        optimizer.zero_grad()

        x = torch.randn(1, 1).to(device)
        y_true = torch.abs(torch.randn(1, 10).to(device))
        y_pred = model(x)

        losses = [
            mse_loss(y_pred, y_true),
            mae_loss(y_pred, y_true),
            huber_loss(y_pred, y_true),
            sid_loss_wrapper(y_pred, y_true).mean(),
            SoftDTWLoss(y_pred.unsqueeze(-1), y_true.unsqueeze(-1)).mean()
        ]
        #print("losses4", losses[4])

        if a in ["GradNorm", "GradNormWithNormalization"]:
            weights = loss_modifier.compute_weights(losses, model)


        if a in ["GradNorm", "GradNormWithNormalization"]:
            L_new = sum(weights[i] * losses[i] for i in range(len(losses)))
            L_new.backward()
            optimizer.step()
            print(f"Epoch {epoch}, Loss: {L_new.item()}, Weights: {weights.cpu().detach().numpy()}")
        elif a == "FirstLossNormalization":
            L_new = loss_modifier.update_weights(losses)
            L_new.sum().backward()
            optimizer.step()