import torch
import torch.nn as nn
import torch.optim as optim
#from chemprop.train.loss_functions import sid_loss
from Graphormer.GP5.Custom_Loss.sdtw_cuda_loss import SoftDTW

from Graphormer.GP5.Custom_Loss.sdtw_python.soft_dtw import soft_dtw

def sid_loss(
    model_spectra: torch.tensor,
    target_spectra: torch.tensor,
    mask: torch.tensor,
    threshold: float = None,
) -> torch.tensor:
    """
    Loss function for use with spectra graphormer_data type.

    :param model_spectra: The predicted spectra output from a model with shape (batch_size,spectrum_length).
    :param target_spectra: The target spectra with shape (batch_size,spectrum_length). Values must be normalized so that each spectrum sums to 1.
    :param mask: Tensor with boolean indications of where the spectrum output should not be excluded with shape (batch_size,spectrum_length).
    :param threshold: Loss function requires that values are positive and nonzero. Values below the threshold will be replaced with the threshold value.
    :return: A tensor containing loss values for the batch with shape (batch_size,spectrum_length).
    """
    # Move new tensors to torch device
    torch_device = model_spectra.device

    # Normalize the model spectra before comparison
    zero_sub = torch.zeros_like(model_spectra, device=torch_device)
    one_sub = torch.ones_like(model_spectra, device=torch_device)
    if threshold is not None:
        threshold_sub = torch.full(model_spectra.shape, threshold, device=torch_device)
        model_spectra = torch.where(
            model_spectra < threshold, threshold_sub, model_spectra
        )
    model_spectra = torch.where(mask, model_spectra, zero_sub)

    sum_model_spectra = torch.sum(model_spectra, axis=1, keepdim=True)
    model_spectra = torch.div(model_spectra, sum_model_spectra)

    # Calculate loss value
    target_spectra = torch.where(mask, target_spectra, one_sub)
    model_spectra = torch.where(
        mask, model_spectra, one_sub
    )  # losses in excluded regions will be zero because log(1/1) = 0.

    loss = torch.mul(
        torch.log(torch.div(model_spectra, target_spectra)), model_spectra
    ) + torch.mul(torch.log(torch.div(target_spectra, model_spectra)), target_spectra)
    return loss

class FirstLossNormalization:
    def __init__(self, epsilon=1e-8, device="cuda"):
        #self.num_losses = num_losses
        self.epsilon = epsilon
        self.device = torch.device(device)

        # Ï≤´ Î≤àÏß∏ ÏÜêÏã§Í∞íÏùÑ Ï†ÄÏû•Ìï† Î≥ÄÏàò (Ï≤´ Î≤àÏß∏ epochÏóê Ï†ÄÏû•Îê®)
        self.first_loss = None

    def update_weights(self, current_losses):
        """
        Ï≤´ Î≤àÏß∏ lossÎßå Í∏∞Ï§ÄÏúºÎ°ú Ï†ïÍ∑úÌôîÌïòÏó¨ Í∞ÄÏ§ëÏπòÎ•º Í≥ÑÏÇ∞.
        """
        current_losses = torch.tensor(current_losses, dtype=torch.float32, device=self.device)

        # Ï≤´ Î≤àÏß∏ epochÏóêÏÑú Ï≤´ Î≤àÏß∏ ÏÜêÏã§ Í∞í Ï†ÄÏû•
        if self.first_loss is None:
            self.first_loss = current_losses.clone().requires_grad_(True) # Ï≤´ Î≤àÏß∏ ÏÜêÏã§Îßå Ï†ÄÏû•

        # Ï≤´ Î≤àÏß∏ ÏÜêÏã§(`self.first_loss`)ÏùÑ Í∏∞Ï§ÄÏúºÎ°ú Ï†ïÍ∑úÌôî
        normalized_losses = current_losses / (self.first_loss + self.epsilon)
        #print(normalized_losses.tolist())
        return normalized_losses

import torch
import torch.nn as nn

class DWA:
    def __init__(self, num_losses, window_size=2, temperature=1.0):
        self.num_losses = num_losses
        self.window_size = window_size
        self.temperature = temperature  # üîπ ÏÜåÌîÑÌä∏Îß•Ïä§ Ïò®ÎèÑ Îß§Í∞úÎ≥ÄÏàò Ï∂îÍ∞Ä
        self.history = torch.zeros((window_size, num_losses)).to("cuda")  # ÏµúÍ∑º loss Í∞í Ï†ÄÏû•
        self.device = torch.device("cuda")

    def update_weights(self, current_losses, epoch):
        """
        Loss Î≥ÄÌôîÏú®ÏùÑ Í∏∞Î∞òÏúºÎ°ú ÏÉàÎ°úÏö¥ Í∞ÄÏ§ëÏπò Í≥ÑÏÇ∞ (ÏÜåÌîÑÌä∏Îß•Ïä§ Ï†ÅÏö©).
        """
        current_losses = torch.tensor(current_losses, dtype=torch.float32, device=self.device)

        # Ï¥àÍ∏∞ Î™á epoch ÎèôÏïàÏùÄ Í∑†Îì± Í∞ÄÏ§ëÏπò ÏÇ¨Ïö©
        if epoch < self.window_size:
            return torch.ones(self.num_losses, dtype=torch.float32, device=self.device) / self.num_losses

        # loss Í∏∞Î°ù ÏóÖÎç∞Ïù¥Ìä∏
        self.history = torch.cat((self.history[1:], current_losses.unsqueeze(0)), dim=0)

        # Loss Î≥ÄÌôîÏú® Í∏∞Î∞ò Í∞ÄÏ§ëÏπò Ï°∞Ï†ï
        ratios = torch.clamp(self.history[-1] / (self.history[-2] + 1e-8), min=0.5, max=2)

        # üîπ ÏÜåÌîÑÌä∏Îß•Ïä§ Ï†ÅÏö©ÌïòÏó¨ Í∞ÄÏ§ëÏπò Í≥ÑÏÇ∞ (Ïò®ÎèÑ Îß§Í∞úÎ≥ÄÏàò Ìè¨Ìï®)
        exp_ratios = torch.exp(-ratios / self.temperature)
        weights = torch.softmax(exp_ratios, dim=0)

        return weights


class DWAWithNormalization:
    def __init__(self, num_losses, window_size=2, epsilon=1e-8, temperature=1.0):
        self.num_losses = num_losses
        self.window_size = window_size
        self.epsilon = epsilon
        self.temperature = temperature  # üîπ ÏÜåÌîÑÌä∏Îß•Ïä§ Ïò®ÎèÑ Îß§Í∞úÎ≥ÄÏàò Ï∂îÍ∞Ä
        self.history = torch.zeros((window_size, num_losses)).to("cuda")  # ÏµúÍ∑º loss Í∞í Ï†ÄÏû•
        self.weights = nn.Parameter(torch.ones(num_losses, dtype=torch.float32))  # ÌïôÏäµ Í∞ÄÎä•Ìïú Í∞ÄÏ§ëÏπò
        self.initial_losses = None  # Ï≤´ Î≤àÏß∏ epochÏùò ÏÜêÏã§ Í∞íÏùÑ Ï†ÄÏû•Ìï† Î≥ÄÏàò
        self.device = torch.device("cuda")

    def update_weights(self, current_losses, epoch):
        """
        Loss Î≥ÄÌôîÏú®ÏùÑ Í∏∞Î∞òÏúºÎ°ú ÏÉàÎ°úÏö¥ Í∞ÄÏ§ëÏπò Í≥ÑÏÇ∞ (Ï¥àÍ∏∞ ÏÜêÏã§ Í∏∞Î∞ò Ï†ïÍ∑úÌôî Ìè¨Ìï® + Softmax Ï†ÅÏö©).
        """
        current_losses = torch.tensor(current_losses, dtype=torch.float32, device=self.device)

        # Ï≤´ Î≤àÏß∏ epochÏóêÏÑú Ï¥àÍ∏∞ ÏÜêÏã§ Í∞í Ï†ÄÏû•
        if self.initial_losses is None:
            self.initial_losses = current_losses.clone().detach()

        # Ï¥àÍ∏∞ ÏÜêÏã§ÏùÑ Í∏∞Ï§ÄÏúºÎ°ú Ï†ïÍ∑úÌôî
        normalized_losses = current_losses / (self.initial_losses + self.epsilon)

        # Ï¥àÍ∏∞ window_size ÎèôÏïà ÏÜêÏã§Í∞í Ï†ÄÏû• Îã®Í≥Ñ
        if epoch < self.window_size:
            self.history[epoch] = normalized_losses.clone().detach()
            return torch.ones(self.num_losses, dtype=torch.float32, device=self.device) / self.num_losses  # Í∑†Îì± Í∞ÄÏ§ëÏπò Î∞òÌôò

        # history ÏóÖÎç∞Ïù¥Ìä∏ (Ïù¥Ï†Ñ Í∞í Ïú†ÏßÄ)
        self.history[:-1] = self.history[1:].clone()
        self.history[-1] = normalized_losses.clone().detach()

        # Loss Î≥ÄÌôîÏú® Í∏∞Î∞ò Í∞ÄÏ§ëÏπò Ï°∞Ï†ï
        ratios = torch.clamp(self.history[-1] / (self.history[-2] + self.epsilon), min=0.5, max=2)

        # üîπ ÏÜåÌîÑÌä∏Îß•Ïä§ Ï†ÅÏö©ÌïòÏó¨ Í∞ÄÏ§ëÏπò Í≥ÑÏÇ∞ (Ïò®ÎèÑ Îß§Í∞úÎ≥ÄÏàò Ìè¨Ìï®)
        exp_ratios = torch.exp(-torch.abs(ratios) / self.temperature)
        self.weights.data = torch.softmax(exp_ratios, dim=0)

        return self.weights

class ModifiedUncertaintyWeightedLoss(nn.Module):
    def __init__(self, num_losses, gamma=0.02):
        super().__init__()
        self.num_losses = num_losses
        self.log_sigmas = nn.Parameter(torch.zeros(num_losses, dtype=torch.float32, requires_grad=True))
        self.gamma = gamma

    def forward(self, losses, epoch):
        """
        Uncertainty Í∏∞Î∞ò Loss Ï°∞Ï†ï
        """
        sigmas = torch.exp(self.log_sigmas)
        decay_factor = torch.exp(torch.tensor(-self.gamma * epoch, dtype=torch.float32))

        losses = torch.tensor(losses, dtype=torch.float32, requires_grad=True)
        total_loss = sum((loss / (2 * sigmas[i] ** 2)) * decay_factor for i, loss in enumerate(losses))
        total_loss += torch.sum(self.log_sigmas)  # Ï†ïÍ∑úÌôî Ìï≠ Ï∂îÍ∞Ä
        return total_loss


class GradNorm:
    def __init__(self, num_losses, alpha=0.12):
        self.num_losses = num_losses # ÏÇ¨Ïö©Ìï† ÏÜêÏã§ Ìï®Ïàò Í∞úÏàò
        self.alpha = alpha # GradNormÏùò Ïä§ÏºÄÏùºÎßÅ Ï°∞Ï†ï Í≥ÑÏàò
        self.lambdas = nn.Parameter(torch.ones(num_losses, dtype=torch.float32, device="cuda")) # ÏÜêÏã§ Í∞ÄÏ§ëÏπò Î≤°ÌÑ∞ (num_losses ÌÅ¨Í∏∞Ïùò ÌïôÏäµ Í∞ÄÎä•Ìïú ÌÖêÏÑú, Ï¥àÍ∏∞Í∞í 1)

    def compute_weights(self, losses, model): # Ïó¨Îü¨ Í∞úÏùò ÏÜêÏã§ Í∞í Î¶¨Ïä§Ìä∏ losses # losses Î¶¨Ïä§Ìä∏ ÏïàÏóêÎäî Ïó¨Îü¨ lossÏùò ÌÖêÏÑúÍ∞Ä Îì§Ïñ¥Í∞ê / Ïòà:[SID, SoftDTW]
        """
        Gradient Norm Í∏∞Î∞ò Í∞ÄÏ§ëÏπò Ï°∞Ï†ï
        """
        grads = []
        for loss in losses:

            # 1. Í∞Å ÏÜêÏã§Ïùò Í∑∏ÎûòÎîîÏñ∏Ìä∏ L2 norm Í≥ÑÏÇ∞
            # model.parameters() Í∞ÄÏ§ëÏπòÏôÄ BiasÎ•º Î∞òÌôò
            # grad Î¶¨Ïä§Ìä∏ÏóêÎäî model.parameters()(Ï¶â, Î™®Îç∏Ïùò Í∞ÄÏ§ëÏπò)ÎßàÎã§ Í≥ÑÏÇ∞Îêú Í∑∏ÎûòÎîîÏñ∏Ìä∏ ÌÖêÏÑúÎì§Ïù¥ Ï†ÄÏû•Îê®
            grad = torch.autograd.grad(loss, model.parameters(), retain_graph=True, create_graph=True, allow_unused=True) # None Í∞íÏù¥ Î∞òÌôòÎê† Í≤ΩÏö∞ Í∏∞Î≥∏Í∞í 0ÏúºÎ°ú Ï≤òÎ¶¨, Í∞Å ÏÜêÏã§Ïóê ÎåÄÌïú Í∑∏ÎûòÎîîÏñ∏Ìä∏Î•º Í≥ÑÏÇ∞


            # 1-1. g.norm() ‚Üí g(Í∞Å Í∞ÄÏ§ëÏπòÏùò Í∑∏ÎûòÎîîÏñ∏Ìä∏ ÌÖêÏÑú)Ïóê ÎåÄÌïú L2 normÏùÑ Í≥ÑÏÇ∞, ÎßåÏïΩ gÍ∞Ä NoneÏù¥Î©¥ torch.tensor(0.0)ÏùÑ Î∞òÌôòÌïòÏó¨ NaN Î∞©ÏßÄ, None Í∞íÏù¥ ÏûàÎäî Ïù¥Ïú†Îäî Ïñ¥Îñ§ ÏÜêÏã§Ïù¥ ÌäπÏ†ï Í∞ÄÏ§ëÏπòÏóê ÏòÅÌñ•ÏùÑ Ï£ºÏßÄ ÏïäÏùÑ ÏàòÎèÑ ÏûàÍ∏∞ ÎïåÎ¨∏
            # 1-2. torch.stack() Î™®Îì† ÎÖ∏Î¶Ñ Í∞íÏùÑ ÌïòÎÇòÏùò ÌÖêÏÑúÎ°ú Î≥ÄÌôò
            # 1-3. ÏµúÏ¢ÖÏ†ÅÏúºÎ°ú L2 normÏùÑ Í≥ÑÏÇ∞ torch.norm(torch.stack([...]))
            # L2 Norm = Ï†úÍ≥± Ìï©ÌïúÌõÑ Ï†úÍ≥±Í∑ºÏùÑ ÏîåÏö∞ÎäîÍ≤É
            grad_norm = torch.norm(torch.stack([g.norm() if g is not None else torch.tensor(0.0) for g in grad]))

            grads.append(grad_norm)

        # 2. Î™®Îì† ÏûëÏóÖÏùò ÌèâÍ∑† Í∑∏ÎûòÎîîÏñ∏Ìä∏ norm Í≥ÑÏÇ∞
        grads = torch.stack(grads) # Î™®Îì† Í∑∏ÎûòÎîîÏñ∏Ìä∏ ÌÅ¨Í∏∞Î•º ÌïòÎÇòÏùò ÌÖêÏÑúÎ°ú Î≥ÄÌôò -> Í∞úÎ≥Ñ loss Îì§Ïùò graident ÌÅ¨Í∏∞
        mean_grad = grads.mean() + 1e-8 # 0 Î∞©ÏßÄ, nÍ∞ú lossÏùò graident ÌèâÍ∑†

        # 3. ÏÉÅÎåÄÏ†Å ÏÜêÏã§ ÎπÑÏú® Í≥ÑÏÇ∞
        relative_losses = losses / (losses[0] + 1e-8)  # Ï¥àÍ∏∞ ÏÜêÏã§ ÎåÄÎπÑ Î≥ÄÌôîÏú® Í≥ÑÏÇ∞
        r_i = relative_losses / (relative_losses.mean() + 1e-8)  # ÏÉÅÎåÄÏ†Å ÏÜêÏã§ ÎπÑÏú®

        # GradNorm ÏóÖÎç∞Ïù¥Ìä∏ Í≥µÏãù Ï†ÅÏö©
        loss_weights = self.lambdas * (grads / mean_grad) ** self.alpha # GradNorm Í≥µÏãù Ï†ÅÏö©

        # Softmax Ï†ïÍ∑úÌôî
        loss_weights = loss_weights / (loss_weights.sum() + 1e-8) # 1e-8 -> 0 Î∞©ÏßÄ
        return loss_weights

    def update_lambdas(self, loss_weights):
        with torch.no_grad():
            self.lambdas.copy_(loss_weights)

class GradNormWithNormalization:
    def __init__(self, num_losses, alpha=0.12, epsilon=1e-8):
        self.num_losses = num_losses
        self.alpha = alpha
        self.epsilon = epsilon
        self.lambdas = nn.Parameter(torch.ones(num_losses, dtype=torch.float32, device="cuda"))  # Ï¥àÍ∏∞ Í∞ÄÏ§ëÏπò 1Î°ú ÏÑ§Ï†ï
        self.initial_losses = None  # Ï¥àÍ∏∞ ÏÜêÏã§ Ï†ÄÏû•

    def compute_weights(self, losses, model):
        # üîπ `losses`Í∞Ä Î¶¨Ïä§Ìä∏ ÌòïÌÉúÏùº Ïàò ÏûàÏúºÎØÄÎ°ú `stack()`ÏúºÎ°ú Î≥ÄÌôò
        losses = torch.stack(losses)

        # üîπ Ï≤´ Î≤àÏß∏ epochÏùò ÏÜêÏã§ Ï†ÄÏû• (Ï¥àÍ∏∞ Í∏∞Ï§ÄÍ∞í)
        if self.initial_losses is None:
            self.initial_losses = losses.clone().detach()

        # üîπ ÏÜêÏã§ Ï†ïÍ∑úÌôî (ÏïàÏ†ïÏ†ÅÏù∏ ÌïôÏäµÏùÑ ÏúÑÌï¥ `epsilon` Ï∂îÍ∞Ä)
        normalized_losses = losses / (self.initial_losses + self.epsilon)
        print("normalized_losses", normalized_losses.tolist())

        # üîπ Î™®Îç∏Ïùò Î™®Îì† ÏÜêÏã§Ïóê ÎåÄÌïú Í∑∏ÎûòÎîîÏñ∏Ìä∏ ÌÅ¨Í∏∞ Í≥ÑÏÇ∞
        grads = []
        for loss in normalized_losses:
            grad = torch.autograd.grad(loss, model.parameters(), retain_graph=True, create_graph=True, allow_unused=True)
            grad_norm = torch.norm(torch.stack([g.norm() if g is not None else torch.tensor(0.0) for g in grad]))
            grads.append(grad_norm)

        grads = torch.stack(grads)  # Í∑∏ÎûòÎîîÏñ∏Ìä∏ ÌÅ¨Í∏∞ ÌÖêÏÑú Î≥ÄÌôò
        mean_grad = grads.mean() + self.epsilon  # ÌèâÍ∑† Í∑∏ÎûòÎîîÏñ∏Ìä∏ ÌÅ¨Í∏∞

        # üîπ GradNorm Í≥µÏãù Ï†ÅÏö© (Ï†ïÍ∑úÌôîÎêú ÏÜêÏã§ & Í∑∏ÎûòÎîîÏñ∏Ìä∏ ÌÅ¨Í∏∞ ÏÇ¨Ïö©)
        loss_weights = self.lambdas * (grads / mean_grad) ** self.alpha

        # üîπ Softmax Ï†ïÍ∑úÌôî Ï†ÅÏö© (Í∞ÄÏ§ëÏπò Ï¥ùÌï©Ïù¥ 1Ïù¥ ÎêòÎèÑÎ°ù)
        loss_weights = loss_weights / (loss_weights.sum() + self.epsilon)
        return loss_weights

if __name__ == '__main__':
    # ‚úÖ Í∞ÑÎã®Ìïú Î™®Îç∏ Ï†ïÏùò (ÏûÖÎ†• 1Í∞ú -> Ï∂úÎ†• 10Í∞ú)
    # ‚úÖ Ïã§Ìñâ ÏÑ§Ï†ï
    a = "DWAWithNormalization"  # "DWA", "DWAWithNormalization", "Uncertainty", "GradNorm", "GradNormWithNormalization" "FirstLossNormalization"Ï§ë ÏÑ†ÌÉù

    # ‚úÖ Î™®Îç∏ Ï†ïÏùò
    class SimpleModel(nn.Module):
        def __init__(self):
            super(SimpleModel, self).__init__()
            self.fc = nn.Linear(1, 10)
            self.activation = nn.Softplus()  # Ìï≠ÏÉÅ ÏñëÏàò Ï∂úÎ†•

        def forward(self, x):
            return self.activation(self.fc(x))  # ÌôúÏÑ±Ìôî Ìï®Ïàò Ï†ÅÏö©


    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = SimpleModel().to(device)
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    # ‚úÖ ÏÜêÏã§ Ìï®Ïàò Ï†ïÏùò
    SoftDTWLoss = SoftDTW(use_cuda=True, gamma=1.0, bandwidth=None)
    mse_loss = nn.MSELoss()
    mae_loss = nn.L1Loss()
    huber_loss = nn.SmoothL1Loss()


    def sid_loss_wrapper(model_spectra, target_spectra):
        mask = torch.ones_like(model_spectra, dtype=torch.bool).to(device)
        SID_LOSS = sid_loss(model_spectra, target_spectra, mask, threshold=1e-6)
        return SID_LOSS


    # ‚úÖ Loss Weighting ÏÑ§Ï†ï
    if a == "DWA":
        loss_modifier = DWA(num_losses=5, window_size=2)
    elif a == "DWAWithNormalization":
        loss_modifier = DWAWithNormalization(num_losses=5, window_size=10)
    elif a == "Uncertainty":
        loss_modifier = ModifiedUncertaintyWeightedLoss(num_losses=5)
    elif a == "GradNorm":
        loss_modifier = GradNorm(num_losses=5, alpha=0.12)
    elif a == "GradNormWithNormalization":
        loss_modifier = GradNormWithNormalization(num_losses=5, alpha=0.12)
    elif a == "FirstLossNormalization":
        loss_modifier = FirstLossNormalization()

    # ‚úÖ ÌïôÏäµ Í≥ºÏ†ï
    num_epochs = 100
    for epoch in range(num_epochs):
        model.train()
        optimizer.zero_grad()

        x = torch.randn(1, 1).to(device)
        y_true = torch.abs(torch.randn(1, 10).to(device))
        y_pred = model(x)

        losses = [
            mse_loss(y_pred, y_true),
            mae_loss(y_pred, y_true),
            huber_loss(y_pred, y_true),
            sid_loss_wrapper(y_pred, y_true).mean(),
            SoftDTWLoss(y_pred.unsqueeze(-1), y_true.unsqueeze(-1)).mean()
        ]

        if a in ["DWA", "DWAWithNormalization"]:
            weights = loss_modifier.update_weights([loss.item() for loss in losses], epoch)
        elif a in ["GradNorm", "GradNormWithNormalization"]:
            weights = loss_modifier.compute_weights(losses, model)
        elif a == "Uncertainty":
            weights = loss_modifier(losses, epoch)

        if a in ["DWA", "DWAWithNormalization","GradNorm", "GradNormWithNormalization","Uncertainty"]:
            L_new = sum(weights[i] * losses[i] for i in range(len(losses)))
            L_new.backward()
            optimizer.step()
            print(f"Epoch {epoch}, Loss: {L_new.item()}, Weights: {weights.cpu().detach().numpy()}")
        elif a == "FirstLossNormalization":
            L_new = loss_modifier.update_weights(losses)
            L_new.sum().backward()
            optimizer.step()
            #print(f"Epoch {epoch}, Loss: {L_new},")




